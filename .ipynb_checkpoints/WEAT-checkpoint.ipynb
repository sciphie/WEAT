{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAT in Python2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class provides all functions that are required for the WEAT method. For default execution only import the present python script and call the function \"call(model, X, Y, A, B, tag = 'none', permutation_num=500000)\" from a external\n",
    "file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the data sets that are used to define the assciation concepts. Here ist is only positive and negative. Sets are named after the terminology in each source paper. Yet, concepts are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association sets from IAT literature\n",
    "\n",
    "# Greenwald et al.(1998) - Measuring Individual Differences In Implicit Cognition - The Implicit Association Test\n",
    "gw_pos = ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure',\n",
    "          'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise',\n",
    "          'family', 'happy', 'laughter', 'paradise', 'vacation']\n",
    "\n",
    "gw_neg = ('abuse crash filth murder sickness accident death grief poison stink assault disaster hatred pollute '\n",
    "          'tragedy bomb divorce jail poverty ugly cancer evil kill rotten vomit agony prison').split()\n",
    "\n",
    "# Nosek, Banaji, Greendwald (2002) - Math Male Me Female Therefore Me Not Math\n",
    "Pleasant_1 = 'assertive athletic strong compassion support sympathetic laughter champion paradise vacation'.split()\n",
    "Unpleasant_1 = 'brutal destroy ruthless confusion insecure naive bad poor waste crude'.split()\n",
    "\n",
    "Pleasant_2 = 'ambition cuddle excitement glory joy love paradise pleasure romantic miracle'.split()\n",
    "Unpleasant_2 = 'agony death detest disaster humiliate jealousy punishment stress tragedy war'.split()\n",
    "\n",
    "Pleasant_3 = 'affectionate cozy enjoyment friend hug laughter passion peace snuggle triumph'.split()\n",
    "Unpleasant_3 = 'afraid crucify despise failure hatred irritate nightmare slap terrible violent'.split()\n",
    "\n",
    "# Nosek et al. (2002) - Harvesting  implicit  group  attitudes  and  beliefs  from  a demonstration  web  site\n",
    "harvest_good = 'Joy Love Peace Wonderful Pleasure Friend Laughter Happy'.lower().split()\n",
    "harvest_bad = 'Agony Terrible Horrible Nasty Evil War Awful Failure Death'.lower().split()\n",
    "\n",
    "# Monteith & Pettit (2011) - Implicit and explicit  stigmatizing  attitudes  and  stereotypes  about  depression.\n",
    "mp_good = 'positive pleasant enjoy glorious wonderful bliss'.split()\n",
    "mp_bad = 'negative horrible agony terrible unpleasant despise'.split()\n",
    "\n",
    "###\n",
    "generalPos = set(Pleasant_1 + Pleasant_2 + Pleasant_3 + gw_pos + harvest_good + mp_good)\n",
    "generalNeg = set(Unpleasant_1 + Unpleasant_2 + Unpleasant_3 + gw_neg + harvest_bad + mp_bad)​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with some imports of course ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "import logging, os\n",
    "from util import *\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained W2V Google Model.\n",
    "The models can be downloaded at https://github.com/mmihaltz/word2vec-GoogleNews-vectors and https://github.com/eyaler/word2vec-slim, respectively. In most cases, the slim version is more than sufficient and much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = models.KeyedVectors.load_word2vec_format('./modelsGoogle/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "m = models.KeyedVectors.load_word2vec_format('./modelsGoogle/word2vec-slim/GoogleNews-vectors-negative300-SLIM.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "​def get_cosines(model, word, word_list):\n",
    "    \"\"\"\n",
    "    Calculate the cosine-similarity of a word to every word in a word list.\n",
    "    Here it is the mean. May also be replaced by other metrices. \n",
    "    :param model:       underlying model\n",
    "    :param word:        target word\n",
    "    :param word_list:   list of words to calculate distance to\n",
    "    :return:            list of cosine similarities, mean cosine similarity\n",
    "    \"\"\"\n",
    "    cosines = []\n",
    "    for elem in word_list:\n",
    "        try:\n",
    "            cosines.append(model.similarity(word, elem))    # Similarity: Compute cosine similarity between two words.\n",
    "        except KeyError:\n",
    "            logging.info('no cosine for ' + word + ' and ' + elem + ' available')\n",
    "\n",
    "    if not cosines:       # Fehler abfangen\n",
    "        logging.error('No cosine values available')\n",
    "        return 0, 0\n",
    "\n",
    "    mean_cosine = sum(cosines) / len(cosines)\n",
    "    return cosines, mean_cosine\n",
    "\n",
    "\n",
    "def s_word(model, w, A, B, out):\n",
    "    \"\"\"\n",
    "    Calculate the association of w with the attribute sets (Is w rather associated to A (positive value) or to B (negative value))\n",
    "    :param model:       underlying model\n",
    "    :param w:           target word\n",
    "    :param A:           association set 1\n",
    "    :param B:           association set 2\n",
    "    :param out:         (boolean) only do prints if true\n",
    "    :return:            s-value\n",
    "    \"\"\"\n",
    "    cosines_wA, mean_cos_wA = get_cosines(model, w, A)\n",
    "    cosines_wB, mean_cos_wB = get_cosines(model, w, B)\n",
    "\n",
    "    s = 0\n",
    "    s_word_val = mean_cos_wA - mean_cos_wB          # negativ, wenn w zu B gehört, sonst positiv\n",
    "    s += s_word_val\n",
    "    if out:\n",
    "        assignment = 'failed (0)'\n",
    "        if s_word_val > 0:\n",
    "            assignment = 'A (' + format(s_word_val, '.4f') + ')'\n",
    "        elif s_word_val < 0:\n",
    "            assignment = 'B (' + format(s_word_val, '.4f') + ')'\n",
    "\n",
    "        info = w + ': A = ' + format(mean_cos_wA, '.4f') + ' ; B = ' \\\n",
    "               + format(mean_cos_wB, '.4f') + ' | assignment: ' + assignment\n",
    "        print(info)\n",
    "    return s_word_val\n",
    "\n",
    "\n",
    "# not  used\n",
    "def s(model, X, Y, A, B, out):\n",
    "    \"\"\"\n",
    "    Calculate the differential association of the two sets of target group with the attribute. This is exactly the same\n",
    "    function as \"s_corrected\" but without normalisation. Thus, this function is no longer used by default.\n",
    "    :param model:       underlying model\n",
    "    :param X:           target set 1\n",
    "    :param Y:           target set 2\n",
    "    :param A:           association set 1\n",
    "    :param B:           association set 2\n",
    "    :param out:         only do prints if true\n",
    "    :return:            s value\n",
    "    \"\"\"\n",
    "    s_xAB_all = []\n",
    "    s_yAB_all = []\n",
    "\n",
    "    for x in X:\n",
    "        if x not in model:\n",
    "            logging.info(x + ' is not in vocabulary -> skip it')\n",
    "        else:\n",
    "            curr = s_word(model, x, A, B, out)             # im Optimalfall positiv\n",
    "            s_xAB_all.append(curr)\n",
    "    if out:\n",
    "        print('~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~')\n",
    "\n",
    "    for y in Y:\n",
    "        if y not in model:\n",
    "            logging.info(y + ' is not in vocabulary -> skip it')\n",
    "        else:\n",
    "            curr = s_word(model, y, A, B, out)             # im Optimalfall negativ\n",
    "            s_yAB_all.append(curr)\n",
    "\n",
    "    if not s_xAB_all or not s_yAB_all:          # Fehler abfangen\n",
    "        logging.error('none of the words is in vocabulary')\n",
    "    sum_sXAB = sum(s_xAB_all)                   # -> Summe positiv\n",
    "    sum_sYAB = sum(s_yAB_all)                   # -> Summe negativ\n",
    "\n",
    "    return sum_sXAB - sum_sYAB      # im Optimalfall ein hoher wert, da zwei Mal -\n",
    "\n",
    "\n",
    "def s_corrected(model, X, Y, A, B, out):\n",
    "    \"\"\"\n",
    "    Calculate the differential association of the two sets of target group with the attribute.\n",
    "    This function is a corrected and thus updated version of s(model, X, Y, A, B, out).\n",
    "    Normalisation allows to insert target word groups with different numbers of elements.\n",
    "    :param model:       underlying model\n",
    "    :param X:           target set 1\n",
    "    :param Y:           target set 2\n",
    "    :param A:           association set 1\n",
    "    :param B:           association set 2\n",
    "    :param out:         only do prints if true\n",
    "    :return:            s value\n",
    "    \"\"\"\n",
    "    s_xAB_all = []\n",
    "    s_yAB_all = []\n",
    "\n",
    "    for x in X:\n",
    "        if x not in model:\n",
    "            logging.info(x + ' in vocabulary (s, x): ' + str(x in model))\n",
    "            print(x, model, x in model)\n",
    "        else:\n",
    "            curr = s_word(model, x, A, B, out)          # im Optimalfall positiv\n",
    "            s_xAB_all.append(curr)\n",
    "    if out:\n",
    "        print('~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~')\n",
    "\n",
    "    for y in Y:\n",
    "        if y not in model:\n",
    "            logging.info(y + ' in vocabulary (s, y): ' + str(y in model))\n",
    "            print(y, model, y in model)\n",
    "        else:\n",
    "            curr = s_word(model, y, A, B, out)          # im Optimalfall negativ\n",
    "            s_yAB_all.append(curr)\n",
    "\n",
    "    if not s_xAB_all or not s_yAB_all:                  # Fehler abfangen\n",
    "        logging.error('none of the words is in vocabulary')\n",
    "        return 0\n",
    "    sum_sXAB = sum(s_xAB_all) / len(X)                  # -> Summe positiv\n",
    "    sum_sYAB = sum(s_yAB_all) / len(Y)                  # -> Summe negativ\n",
    "\n",
    "    return sum_sXAB - sum_sYAB                          # im Optimalfall ein hoher wert, da zwei Mal -\n",
    "\n",
    "\n",
    "def effect_size(model, X, Y, A, B):\n",
    "    \"\"\"\n",
    "    calculate the effect size of association\n",
    "    :param X:           target set 1\n",
    "    :param Y:           target set 2\n",
    "    :param A:           association set 1\n",
    "    :param B:           association set 2\n",
    "    :param model:       underlying model\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s_values_x = []\n",
    "    s_values_y = []\n",
    "\n",
    "    for x in X:\n",
    "        if x not in model:\n",
    "            logging.info(x + ' in vocabulary (effect_size, x): ' + str(x in model))\n",
    "        else:\n",
    "            s_values_x.append(s_word(model, x, A, B, False))\n",
    "    for y in Y:\n",
    "        if y not in model:\n",
    "            logging.info(y + ' in vocabulary (effect_size, y): ' + str(y in model))\n",
    "        else:\n",
    "            s_values_y.append(s_word(model, y, A, B, False))\n",
    "\n",
    "    if not s_values_x or not s_values_y:                # Fehler abfangen\n",
    "        logging.error('non of the words is in vocabulary')\n",
    "        return 0\n",
    "\n",
    "    mean_s_val_x = sum(s_values_x) / len(s_values_x)\n",
    "    mean_s_val_y = sum(s_values_y) / len(s_values_y)\n",
    "\n",
    "    s_values_all = s_values_x + s_values_y\n",
    "    mean_s_val_all = sum(s_values_all) / len(s_values_all)\n",
    "    s_values_all_corrected = s_values_all\n",
    "    s_values_all_corrected[:] = [((x - mean_s_val_all)**2) for x in s_values_all]        # iterable: (x_i - mean(x))²\n",
    "    std_dev = math.sqrt((1 / (len(s_values_all) - 1)) * sum(s_values_all_corrected))\n",
    "\n",
    "    return (mean_s_val_x - mean_s_val_y) / std_dev\n",
    "\n",
    "\n",
    "def permutation_test(model, X, Y, A, B, tag, permutation_num):\n",
    "    \"\"\"\n",
    "    do s(X,Y,A,B) for all possible permutations of X & Y to check whether they are lower than weat\n",
    "    :param model:           underlying model\n",
    "    :param X:               target set 1\n",
    "    :param Y:               target set 2\n",
    "    :param A:               association set 1\n",
    "    :param B:               association set 2\n",
    "    :param tag:             (string) name for the saved logfile (no file extension)\n",
    "    :param permutation_num: Max number of random permutations\n",
    "    :return:                weat value, p value, number of permutations with higher weat value,\n",
    "                            ~ with equal ~, ~ with lower weat value\n",
    "    \"\"\"\n",
    "    all_target_words = X + Y\n",
    "    n = len(all_target_words)\n",
    "    if (n%2) == 1:\n",
    "        logging.error('ungerade Anzahl von Target Words im Spiel!')\n",
    "    all_permutations = []\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    full = False\n",
    "    if n <= 15:\n",
    "        full = True\n",
    "        logging.info('Do permutation test with full permutation.')\n",
    "        combinations = list(itertools.combinations(all_target_words, len(X)))\n",
    "        combinations = combinations[1:]               # remove second half and default order\n",
    "\n",
    "        controlIter = 0\n",
    "        for combination in combinations:\n",
    "            controlIter += 1\n",
    "\n",
    "            rest = all_target_words.copy()\n",
    "            for elem in combination:\n",
    "                rest.remove(elem)\n",
    "            all_permutations.append([list(combination), rest])\n",
    "\n",
    "    else:\n",
    "        iterations = permutation_num\n",
    "        logging.info('Too many elements for full permutation. Do random sampling. Iterations: ' + str(iterations))\n",
    "\n",
    "        for _ in itertools.repeat(None, iterations):\n",
    "            shuffle = numpy.random.permutation(all_target_words).tolist()\n",
    "            half1 = shuffle[:n // 2]\n",
    "            half2 = shuffle[n // 2:]\n",
    "            all_permutations.append([half1,half2])\n",
    "            k += 1\n",
    "        # logging.info('permutations done. number: ' + str(len(all_permutations)))\n",
    "\n",
    "    i = 0\n",
    "    n_2 = len(all_permutations)\n",
    "    higher = 0\n",
    "    equal = 0\n",
    "    lower = 0\n",
    "\n",
    "    # logging.info('permutation test: call s-corrected')\n",
    "    weat = s_corrected(model, X, Y, A, B, False)\n",
    "    # logging.info('done: call s-corrected. result: ' + str(weat))\n",
    "    plot_vals = []\n",
    "\n",
    "    if not full:\n",
    "        logging.info('betrachte Permutations-P-Werte nur im Betrag')\n",
    "    logging.info('start loop')\n",
    "    for permutation in all_permutations:\n",
    "        s_val = s_corrected(model, permutation[0], permutation[1], A, B, False)\n",
    "        if not full:\n",
    "            s_val = abs(s_val)          # Betrachte nur Samples aus dem positiven Raum und versopple so die Anzahl\n",
    "        plot_vals.append(s_val)\n",
    "        bar(i, n_2, 50, \"P value calculation: \")\n",
    "        if s_val > weat:\n",
    "            higher += 1\n",
    "        elif weat > s_val:\n",
    "            lower += 1\n",
    "        elif weat == s_val:\n",
    "            equal += 1\n",
    "        else:\n",
    "            logging.error('hier stimmt was nicht!')\n",
    "        i += 1\n",
    "\n",
    "    p_value = 1 - (lower / (higher + equal + lower))       # observed or greater difference\n",
    "    plt_hist(plot_vals, tag, weat)\n",
    "    return weat, p_value, higher, equal, lower\n",
    "\n",
    "\n",
    "def plt_hist(list, name, weat=None, bins=30):\n",
    "    \"\"\"\n",
    "    plot permutation stats as histogram to visualise significance of result. Automatically saves the plot\n",
    "    :param list:        list of weat-values to plot\n",
    "    :param name:        (string) name for log file (no file extension)\n",
    "    :param weat:        (number) actual weat value\n",
    "    :param bins:        bin size for histograms\n",
    "    \"\"\"\n",
    "    plt.hist(list, bins)\n",
    "    list.sort()\n",
    "    if weat:\n",
    "        plt.axvline(weat, color='r')\n",
    "    print(list)\n",
    "\n",
    "    plt.title('permutation test values')\n",
    "    plt.xlabel('value')\n",
    "    plt.savefig('./plots/plt_' + name + '.pdf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
